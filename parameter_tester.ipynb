{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitaudiosegmenternxtome9venv14dc4fbf5d3b4babbd14231c8ff30d50",
   "display_name": "Python 3.8.5 64-bit ('Audio_segmenter-NxT_OME9': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "from pydub.silence import detect_nonsilent\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import noisereduce as nr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this script to test and find the best silence lens and threshold\n",
    "# Splices fragment of audio file\n",
    "#TODO https://stackoverflow.com/questions/59102171/getting-timestamps-from-audio-using-python\n",
    "#TODO add directory\n",
    "\n",
    "\n",
    "#Input directory example\n",
    "#/Users/michael/Desktop/test_demo.m4a\n",
    "\n",
    "# guide\n",
    "# target_length: If chunk length < than 5 secs, it will attempt to pad the chunk by retaining natural silence\n",
    "# min_silence_len: Determine how long silences/pauses are too long and unneccessary \n",
    "# Silence_thresh: anything under the defined dBFS is considered silence. The lower the silence threshold, the less sensitive it can pick silence\n",
    "\n",
    "\n",
    "#Parameters\n",
    "seconds = 1000 # 1000 ms\n",
    "predefined_ms = 60 * seconds\n",
    "target_dBFS = -20.0\n",
    "min_silence_len = 4 * 1000\n",
    "target_silence_thresh = -38\n",
    "channels = 1 # global variable\n",
    "frame_rate = 22050 # global variable\n",
    "seek_step = 1\n",
    "\n",
    "audio_path = input(\"Input audio path\") #path refers to the file\n",
    "loaded_audio = AudioSegment.from_file(audio_path, format=\"m4a\")\n",
    "\n",
    "#Splices a section of the audio for testing\n",
    "# spliced_test_audio = loaded_audio[:predefined_ms] #first 60 seconds\n",
    "# spliced_test_audio = loaded_audio[(predefined_ms*1):(predefined_ms * 2)] # second 60 seconds\n",
    "# spliced_test_audio = loaded_audio[(predefined_ms *2):(predefined_ms * 3)]  # third 60 seconds\n",
    "spliced_test_audio = loaded_audio[(predefined_ms *3):(predefined_ms * 4)]  # fourth 60 seconds\n",
    "\n",
    "\n",
    "#Splices the last 60 seconds of the audio\n",
    "# spliced_test_audio = loaded_audio[-predefined_ms:] \n",
    "\n",
    "#change to monochannel and change sampling rate\n",
    "spliced_test_audio  = spliced_test_audio.set_channels(channels)\n",
    "spliced_test_audio = spliced_test_audio.set_frame_rate(frame_rate)\n",
    "\n",
    "#Export spliced sample audio\n",
    "spliced_test_audio.export(\"test_sample.wav\", format=\"wav\")# Use this script to test and find the best \n",
    "print(\"Audio successfully spliced\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Use this to adjust parameters. Check if the start and stop times correspond with wave plot.\n",
    "\n",
    "#Convert test wav to audio_segment\n",
    "audio_segment = AudioSegment.from_wav(\"test_sample.wav\")\n",
    "\n",
    "#Convert seconds into timestamp\n",
    "def convert(seconds): \n",
    "    seconds = seconds % (24 * 3600) \n",
    "    hour = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "      \n",
    "    return \"%d:%02d:%02d\" % (hour, minutes, seconds) \n",
    "\n",
    "#adjust target amplitude\n",
    "def match_target_amplitude(sound, target_dBFS):\n",
    "    change_in_dBFS = target_dBFS - sound.dBFS\n",
    "    return sound.apply_gain(change_in_dBFS)\n",
    "\n",
    "#normalize audio_segment to -20dBFS \n",
    "normalized_sound = match_target_amplitude(audio_segment, target_dBFS)\n",
    "audio_length = len(normalized_sound)/1000 \n",
    "print(\"length of audio_segment={} seconds\".format(audio_length))\n",
    "\n",
    "#Export and display normalized amplitude file\n",
    "normalized_sound.export(\"test_sample_normalized.wav\", format=\"wav\")\n",
    "\n",
    "#Print detected non-silent chunks, which in our case would be spoken words.\n",
    "#the lower the silence threshold (higher negative number), the less senstive it is\n",
    "nonsilent_data = detect_nonsilent(normalized_sound, min_silence_len=min_silence_len, silence_thresh=target_silence_thresh, seek_step=seek_step)\n",
    "\n",
    "#convert ms to seconds\n",
    "print(\"start,Stop\")\n",
    "for chunks in nonsilent_data:\n",
    "    print([convert(chunk/1000) for chunk in chunks])\n",
    " \n",
    " \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Graphs the sample audio. Used to see if the silence parameters are acceptable\n",
    "#ALso use\n",
    "\n",
    "#Plots the sample test audio\n",
    "test_dir = \"test_sample.wav\"\n",
    "audio_data, sampling_rate = librosa.load(test_dir)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.xticks(np.arange(min(audio_data), max(audio_data)+1, 1.0))\n",
    "librosa.display.waveplot(audio_data, sr=sampling_rate)\n",
    "\n",
    "#Plots the normalized test audio\n",
    "test_dir = \"test_sample_normalized.wav\"\n",
    "audio_data, sampling_rate = librosa.load(test_dir)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveplot(audio_data, sr=sampling_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Reduces noise and graphs the sample. Compare this graph above to see if its acceptable.\n",
    "\n",
    "# #Determine which part of the audio can be used as a reference for background noise\n",
    "# # done in ms / 1000m = 1 second\n",
    "# noisy_part = audio_data[22000:26000]\n",
    "# # perform noise reduction\n",
    "# reduced_noise = nr.reduce_noise(audio_clip=audio_data,\n",
    "# noise_clip=noisy_part, verbose=True)\n",
    "\n",
    "# plt.figure(figsize=(14, 5))\n",
    "# librosa.display.waveplot(audio_data, sr=sampling_rate)\n",
    "\n",
    "\n"
   ]
  }
 ]
}